---
layout: post
title: "机器学习中的数学：（二）解析几何"
date: 2021-04-22 07:17:46 +0000
categories: [blog]
tags: []
---

# [](#%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95-Analytic-Geometry)解析几何(Analytic Geometry)

这章将从几何的角度理解之前提及的一些概念。

## [](#%E8%8C%83%E6%95%B0-Norm)范数(Norm)

范数实际上就是向量的一个长度
<!-- Image removed: CSDN link no longer accessible -->
范数有以下性质：
<!-- Image removed: CSDN link no longer accessible -->

- 第一个绝对齐次（？）实际上数量积不就是对向量长度的一个延伸，所以，缩放的量可以提出来。
- 第二个三角不等式，因为两个向量和这两个向量的向量和会形成一个三角形，三角形有一个性质就是两边之和大于等于第三边
- 最后一个是因为长度是非负的

下面是两种不同的范数，这种区别是对距离的定于不同导致的。

### [](#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%8C%83%E6%95%B0%EF%BC%88Manhattan-Norm%EF%BC%89)曼哈顿范数（Manhattan Norm）

<!-- Image removed: CSDN link no longer accessible -->
由上图可以了解到曼哈顿距离和欧几里得距离的区别，这样曼哈顿距离就是对应的向量（坐标）所有元素的绝对值之和。其实就是点在水平和竖直方向的位移总和。(表示向量的元素，表示绝对值)
<!-- Image removed: CSDN link no longer accessible -->
表示方式：

### [](#%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%8C%83%E6%95%B0%EF%BC%88Euclidean-Norm%EF%BC%89)欧几里得范数（Euclidean Norm）

这个使用的就是直观的“直线距离”：
<!-- Image removed: CSDN link no longer accessible -->
表示方式：

曼哈顿范数（左）和欧几里得范数（右）的实例：
<!-- Image removed: CSDN link no longer accessible -->

## [](#%E5%86%85%E7%A7%AF%EF%BC%88Inner-Product%EF%BC%89)内积（Inner Product）

内积可以理解为，两个向量在同一向量空间（转换后）下的长度的乘积。
`点积`：两维度相同的向量相乘最后得到一个实数。

$$x^\top y = \sum_{i=1}^{n}x_iy_i$$

点积的几何含义：
<!-- Image removed: CSDN link no longer accessible -->
所以从图像上看，可以得到部分点积的性质：当两向量相反的时候，点积为负数；当两向量垂直的时候，点积为0（在另一个向量的投影的长度为0）。当两向量方向相同的时候，点积为正。
内积的齐次性和对称性：两个向量哪个投影至哪个其实并没有什么区别，所以，二者乘积的顺序是无关紧要的。

> 

点积为什么是这样计算的？
<!-- Image removed: CSDN link no longer accessible -->
<!-- Image removed: CSDN link no longer accessible -->

### [](#%E5%B9%BF%E4%B9%89%E5%86%85%E7%A7%AF)广义内积

`双线性映射`（bilinear mapping）
<!-- Image removed: CSDN link no longer accessible -->
当映射的参数顺序交换后，映射结果保持一致，这种性质称为`对称`(symmetric).当映射结果不会小于0， 这种性质称为`正定`（positive definite）
<!-- Image removed: CSDN link no longer accessible -->
这样，内积的广义定义就是一个正定、对称的双线性映射。
<!-- Image removed: CSDN link no longer accessible -->

> 

内积空间是不是就是向量空间中两两通过运算之后得到一个实数的向量组成的空间？理解一下上图最后一句化的含义。
<!-- Image removed: CSDN link no longer accessible -->

### [](#%E5%AF%B9%E7%A7%B0%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5%EF%BC%88Symmetric-Positive-Definite-Matrices%EF%BC%89)对称正定矩阵（Symmetric, Positive Definite Matrices）

<!-- Image removed: CSDN link no longer accessible -->

由于内积是正定的，所以有上式可以得出：

\forall x \in V \backslash \{0\}:x^T\bold Ax > 0

是任意的非零向量。
对于一个满足上式的对称矩阵，称为**正定矩阵**

\forall x \in V \backslash \{0\}:x^T\bold Ax \ge 0

满足上式的对称矩阵称为**半正定矩阵**

可以使用一个正定矩阵定义一个内积：
<!-- Image removed: CSDN link no longer accessible -->
<!-- Image removed: CSDN link no longer accessible -->
因为矩阵正定，所以。这一就是说，所以A的零空间只能是。同时，对角线的元素都大于0，原因如下：
<!-- Image removed: CSDN link no longer accessible -->

## [](#%E9%95%BF%E5%BA%A6%E4%B8%8E%E8%B7%9D%E7%A6%BB%EF%BC%88Lengths-and-Distances%EF%BC%89)长度与距离（Lengths and Distances）

内积和范数之间的关系十分紧密。这样理解，（在欧氏几何内）内积其实就是一个向量在另一个向量上投影之后，得到的向量，这两个向量的长度的乘积就是内积。范数简单来说就是向量的长度。所以，两个相同的向量的内积就是这个向量的范数的平方。

$$\|x\| := \sqrt {\langle x, x\rangle}$$

**柯西-施瓦茨不等式**（Cauchy-Schwarz Inequality）：
<!-- Image removed: CSDN link no longer accessible -->
对于这个公式用图形非常好理解：不等式左边是投影之后的两向量的乘积（见之前点积部分介绍的投影），而右边是两向量没有经过投影的长度乘积。而只有两向量相等的时候，一个向量投影到另一个向量不会损失长度，这时候不等式取得等号，否则投影之后的向量长度都会变小。
在欧几里得空间中有特例：
<!-- Image removed: CSDN link no longer accessible -->
**距离和度规**（Distance and Metric）：
**距离**的定义：
<!-- Image removed: CSDN link no longer accessible -->
**度规**的定义：
在数学中，度量（度规）或距离函数是个函数，定义了集合内每一对元素之间的距离。
<!-- Image removed: CSDN link no longer accessible -->

度规和内积有类似的性质，但是他们在某方面又是不同的。当两个向量越接近的时候，内积越大，而度规越小。

## [](#%E5%A4%B9%E8%A7%92%E4%B8%8E%E6%AD%A3%E4%BA%A4%E6%80%A7-Angles-and-Orthogonality)夹角与正交性(Angles and Orthogonality)

内积可以用于定义**两向量的夹角**：
由之前提到的的柯西-施瓦茨不等式：|\langle\boldsymbol{x}, \boldsymbol{y}\rangle| \leqslant\|\boldsymbol{x}\|\|\boldsymbol{y}\|可以得到：

$$-1 \leqslant \frac{\langle\boldsymbol{x}, \boldsymbol{y}\rangle}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|} \leqslant 1$$

<!-- Image removed: CSDN link no longer accessible -->
<!-- Image removed: CSDN link no longer accessible -->在这个范围内，余弦函数的单调的。用来表示两个向量的相近程度。
内积更重要的是可以为定义**两向量的正交性**:
<!-- Image removed: CSDN link no longer accessible -->
两向量正交实际上就是他们之间的夹角为,这时候的余弦值为0，由

$$\cos \omega=\frac{\langle\boldsymbol{x}, \boldsymbol{y}\rangle}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|}$$

因为和都是正定的，所以当是，等于0.当x、y的范数（长度）为1时，称为**规范化正交**(orthonormal).当一个向量是时，它与所有的向量都正交。
正交依赖于内积，所以在不同的内积的情况下，正交性可能不同。

**正交矩阵**
<!-- Image removed: CSDN link no longer accessible -->

> 

转置矩阵的变换关系？

$$\|A x\|^{\top}=(A x)^{\top}(A x)=x^{\top} A^{\top} A x=x^{\top} \boldsymbol{I} x=x^{\top} x=\|x\|^{2}\cos \omega=\frac{(\boldsymbol{A} \boldsymbol{x})^{\top}(\boldsymbol{A} \boldsymbol{y})}{\|\boldsymbol{A} \boldsymbol{x}\|\|\boldsymbol{A} \boldsymbol{y}\|}=\frac{\boldsymbol{x}^{\top} \boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{y}}{\sqrt{\boldsymbol{x}^{\top} \boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{x} \boldsymbol{y}^{\top} \boldsymbol{A}^{\top} \boldsymbol{A} \boldsymbol{y}}}=\frac{\boldsymbol{x}^{\top} \boldsymbol{y}}{\|\boldsymbol{x}\|\|\boldsymbol{y}\|}$$

由上可知，向量在经过正交变换之后，他们之间的夹角和长度都没有发生变化，实际上，正交变换就是将向量进行旋转操作。

### [](#%E8%A7%84%E8%8C%83%E6%AD%A3%E4%BA%A4%E5%9F%BA%EF%BC%88Orthonormal-Basis%EF%BC%89)规范正交基（Orthonormal Basis）

<!-- Image removed: CSDN link no longer accessible -->
一对规范正交基满足两个条件，二者之间的夹角和他们各自的长度。 规范（长度为1）且正交（两对基相互垂直）

> 

**格拉姆-施密特正交化 Gram–Schmidt process**
这里时利用高斯消元法来取得正交规范正交基<!-- Image removed: CSDN link no longer accessible -->

### [](#%E6%AD%A3%E4%BA%A4%E8%A1%A5%EF%BC%88Orthogonal-Complement%EF%BC%89)正交补（Orthogonal Complement）

<!-- Image removed: CSDN link no longer accessible -->
一个向量空间的两个子空间，这两个子空间的维度之和等于原先的向量空间的维度，准确来说，一个子空间占领原空间的部分维度，另一个子空间占领剩余的维度，二者在维度上没有关系。
一个实例
<!-- Image removed: CSDN link no longer accessible -->

这样，原先向量空间中的任意向量，都可以用这个子空间的有序基以及其正交补的有序基表示出来（分解）：

$$\boldsymbol{x}=\sum_{m=1}^{M} \lambda_{m} \boldsymbol{b}_{m}+\sum_{j=1}^{D-M} \psi_{j} \boldsymbol{b}_{j}^{\perp}, \quad \lambda_{m}, \psi_{j} \in \mathbb{R}$$

其中，是原先的向量空间的一个向量，是原先空间的一个子空间的有序基，是这个子空间的正交补的有序基。
<!-- Image removed: CSDN link no longer accessible -->

## [](#%E5%87%BD%E6%95%B0%E7%9A%84%E5%86%85%E7%A7%AF)函数的内积

有之前的点积：

$$x^Ty = \sum_{i = 1}^nx_iy_i$$

当向量的维度有无限维时，可以将这个利用定积分的定义，写成积分形式。

$$\int_{a}^{b}f(x) = \lim_{\lambda \rarr 0}\sum_{i=1}^nf(\xi_i)\Delta x_i,\quad \lambda = max\{\Delta x_1,\Delta x_2,...,\Delta x_n\}$$

从而：
<!-- Image removed: CSDN link no longer accessible -->

当两个函数在一定区间上的定积分为0时，说这两个函数时正交函数。

> 

所有的正交函数够成的一个子空间<!-- Image removed: CSDN link no longer accessible -->
想要正确理解这个无穷维向量的内积，需要将积分延伸到希尔伯特空间（Hilbert space）中。

## [](#%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1%EF%BC%88Orthogonal-Projections%EF%BC%89)正交投影（Orthogonal Projections）

在机器学习中，由于研究对象通常由多标签组成的，所以就不得不使用高维矩阵，但是实际上，大多数的信息仅仅存储在少部分的标签中，所以，当需要对矩阵进行可视化或者数据压缩的时候，为了减少造成的信息损失，可以使用正交投影，这样压缩之后的数据损失最小。
下面是对投影的定义：
<!-- Image removed: CSDN link no longer accessible -->

> 

怎么理解？
应该是对一个向量进行两次投影的与进行一次投影的效果是一致的。 假设一个向量被正交投影到向量空间V中，然后再被正交投影到W中，那么这个向量可以直接利用一次正交变换投影到W中.
类似于
<!-- Image removed: CSDN link no longer accessible -->

投影本质上就是一种对向量的变换，所以可以用矩阵来描述，所以投影操作对应的矩阵就是**投影矩阵**（projection matrices，）

### [](#%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1%E5%88%B0%E4%B8%80%E7%BB%B4%E5%AD%90%E7%A9%BA%E9%97%B4)正交投影到一维子空间

可以通过以下三步求解投影矩阵：
<!-- Image removed: CSDN link no longer accessible -->

**1.找到坐标:**

$$\left\langle\boldsymbol{x}-\pi_{U}(\boldsymbol{x}), \boldsymbol{b}\right\rangle=0 \stackrel{\pi_{U}(\boldsymbol{x})=\lambda \boldsymbol{b}}{\Longleftrightarrow}\langle\boldsymbol{x}-\lambda \boldsymbol{b}, \boldsymbol{b}\rangle=0$$
$$
注意到$\boldsymbol{x}-\pi_{U}(\boldsymbol{x})$是向量及其投影向量做差之后得到的向量，所以与投影到的向量正交。因为投影之后的向量属于向量空间U，所以可以用U中的有序基线性$\bold b$表示。
$$
$$\langle\boldsymbol{x}, \boldsymbol{b}\rangle-\lambda\langle\boldsymbol{b}, \boldsymbol{b}\rangle=0 \Longleftrightarrow \lambda=\frac{\langle\boldsymbol{x}, \boldsymbol{b}\rangle}{\langle\boldsymbol{b}, \boldsymbol{b}\rangle}=\frac{\langle\boldsymbol{b}, \boldsymbol{x}\rangle}{\|\boldsymbol{b}\|^{2}} .$$

这里是利用了内积的双线性的性质，将原先的式子进行了拆分，最后的等式是利用了内积的对称性。之后分离出，任务完成。

$$\lambda=\frac{\boldsymbol{b}^{\top} \boldsymbol{x}}{\boldsymbol{b}^{\top} \boldsymbol{b}}=\frac{\boldsymbol{b}^{\top} \boldsymbol{x}}{\|\boldsymbol{b}\|^{2}}$$

（这里探究当内积为点积的情况）

**2.找到投影点（投影后的向量）：**

$$\pi_{U}(\boldsymbol{x})=\lambda \boldsymbol{b}=\frac{\langle\boldsymbol{x}, \boldsymbol{b}\rangle}{\|\boldsymbol{b}\|^{2}} \boldsymbol{b}=\frac{\boldsymbol{b}^{\top} \boldsymbol{x}}{\|\boldsymbol{b}\|^{2}} \boldsymbol{b}$$

将之前的结果带入式中，最后的等式为当内积为点积的时候成立。

$$\left\|\pi_{U}(\boldsymbol{x})\right\| \stackrel{(3.42)}{=} \frac{\left|\boldsymbol{b}^{\top} \boldsymbol{x}\right|}{\|\boldsymbol{b}\|^{2}}\|\boldsymbol{b}\| \stackrel{(3.25)}{=}|\cos \omega|\|\boldsymbol{x}\|\|\boldsymbol{b}\| \frac{\|\boldsymbol{b}\|}{\|\boldsymbol{b}\|^{2}}=|\cos \omega|\|\boldsymbol{x}\| .$$

点积为内积的情况下,同时，联立了

**3.找到投影矩阵**

$$\pi_{U}(\boldsymbol{x})=\lambda \boldsymbol{b}=\boldsymbol{b} \lambda=\boldsymbol{b} \frac{\boldsymbol{b}^{\top} \boldsymbol{x}}{\|\boldsymbol{b}\|^{2}}=\frac{\boldsymbol{b} \boldsymbol{b}^{\top}}{\|\boldsymbol{b}\|^{2}} \boldsymbol{x}$$

于是：

$$\bold P_\pi = \frac{\bold b\bold b^T}{\|\bold b\|^2}$$

这样看投影矩阵就是一个对称矩阵。

### [](#%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1%E5%88%B0%E4%B8%80%E8%88%AC%E7%9A%84%E5%AD%90%E7%A9%BA%E9%97%B4)正交投影到一般的子空间

假设一个子空间,因为投影的向量属于U，所以，这个投影向量可以用U的有序基表示出来：

$$\bold \pi_U(\bold x) =\sum\limits_{i=1}^m\lambda_i\bold b_i$$

**1.找出投影的坐标**:

$$\bold\pi_U(\bold x) = \sum\limits_{i=1}^m\lambda_i\bold b_i = \bold B\bold\lambda\\\bold B=[\bold b_1,...,\bold b_m]\in\mathbb R^{n\times m},\quad\lambda=[\lambda_1,...,\lambda_m]^T\in\mathbb R^m$$

假设内积为点乘：

$$\left\langle\boldsymbol{b}_{1}, \boldsymbol{x}-\pi_{U}(\boldsymbol{x})\right\rangle=\boldsymbol{b}_{1}^{\top}\left(\boldsymbol{x}-\pi_{U}(\boldsymbol{x})\right)=0\\\vdots\\\left\langle\boldsymbol{b}_{m}, \boldsymbol{x}-\pi_{U}(\boldsymbol{x})\right\rangle=\boldsymbol{b}_{m}^{\top}\left(\boldsymbol{x}-\pi_{U}(\boldsymbol{x})\right)=0$$

由,带入到上式中：

$$\bold b^T_1(\bold x - \bold B\bold\lambda)=0\\\vdots\\\bold b^T_m(\bold x-\bold B\lambda)=0$$

转换成矩阵形式：

\begin{aligned}
$$\left[\begin{array}{c}$$
$$b_{1}^{\top} \\$$
\vdots \\
$$b_{m}^{\top}$$
$$\end{array}\right][x-B \lambda]=0 & \Longleftrightarrow B^{\top}(x-B \lambda)=0$$
$$& \Longleftrightarrow B^{\top} B \lambda=B^{\top} x .$$
$$\end{aligned}$$

因为是U的有序基，所以他是可逆的，所以可以得到：

$$\lambda=(\bold B^T\bold B)^{-1}\bold B^T\bold x$$

其中：称为伪逆，可以用于计算非方阵矩阵。
**2.找到投影向量：**
由,带入上式：

$$\pi_U(x) = \bold B(\bold B^T\bold B)^{-1}\bold B^T\bold x$$

**3.找到投影矩阵：**
由,由上式可以得出：

$$\bold P_\pi=\bold B(\bold B^T\bold B)^{-1}\bold B^T>$$

原始向量与投影向量之差够成的向量的范数，称为**重构误差**（reconstruction error.）或者投影误差。<!-- Image removed: CSDN link no longer accessible -->

虽然说但是我们只需要用U的有序基就可以表示

用正交投影可以用于求非齐次方程无解的时候的近似解。当这个方程无解的时候，说明和不在同一个向量空间中，所以无法通过一些变换（）得到。这时候可以利用正交投影，将其中一个向量投影到另一个向量的向量空间中，这样可以得到一个近似解，其中的主要思想就是找到一个在A的张成空间中，与b最相近的向量。这样得到的解称为**最小二乘解**（least-squares solution）

### [](#%E6%A0%BC%E6%8B%89%E5%A7%86-%E6%96%BD%E5%AF%86%E7%89%B9%E6%AD%A3%E4%BA%A4%E5%8C%96%EF%BC%88Gram-Schmidt-Orthogonalization%EF%BC%89)格拉姆-施密特正交化（Gram-Schmidt Orthogonalization）

<!-- Image removed: CSDN link no longer accessible -->

> 

这里的目标是求出，利用已知的数据计算出这样就可以利用计算了。

我们可以使用向量以及其投影所在的向量空间的有序基作差，得到一个法向量。然后递归地将有序基转化成正交基。

$$\bold {\mathcal u}:=\bold b_1 \\ \mathcal u_k:=\bold b_k- \pi_{span[\bold u_1,\dots,\bold u_{k-1}]}(\bold b_k),\quad k =2,\dots,n$$

其中，是之前缔造的正交向量组成的向量空间（）

### [](#%E5%9C%A8%E4%BB%BF%E5%B0%84%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%AD%A3%E4%BA%A4%E6%8A%95%E5%BD%B1)在仿射空间中的正交投影

<!-- Image removed: CSDN link no longer accessible -->
$$先将目标向量与支撑点()相减，得到的向量就是以仿射空间为起点的，这时候，问题就转换成我们之前讨论过的问题了。\pi_L(\bold x)=\bold x_0+\pi_U(\bold x-\bold x_0)$$
<!-- Image removed: CSDN link no longer accessible -->

## [](#%E6%97%8B%E8%BD%AC%E5%8F%98%E6%8D%A2)旋转变换

旋转实际上就是一种正交变换。在文中规定当旋转角度为正数的时候，图像作逆时针旋转。
<!-- Image removed: CSDN link no longer accessible -->

### [](#%E5%9C%A8%E4%BA%8C%E7%BB%B4%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%97%8B%E8%BD%AC%EF%BC%88Rotations-in-mathbb-R-2-%EF%BC%89)在二维实空间中的旋转（Rotations in 

）
<!-- Image removed: CSDN link no longer accessible -->
因为旋转之后的基向量仍然是线性无关的，所以，旋转也是一种基变换。由上可以得到旋转矩阵（旋转之后的向量）：

$$\Phi\left(\boldsymbol{e}_{1}\right)=\left[\begin{array}{c}\cos \theta \\ \sin \theta\end{array}\right], \quad \Phi\left(\boldsymbol{e}_{2}\right)=\left[\begin{array}{c}-\sin \theta \\ \cos \theta\end{array}\right]\boldsymbol{R}(\theta)=\left[\begin{array}{ll}\Phi\left(\boldsymbol{e}_{1}\right) & \Phi\left(\boldsymbol{e}_{2}\right)\end{array}\right]=\left[\begin{array}{cc}\cos \theta & -\sin \theta \\ \sin \theta & \cos \theta\end{array}\right] .### [](#%E5%9C%A8%E4%B8%89%E7%BB%B4%E5%AE%9E%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%97%8B%E8%BD%AC-Rotations-in-mathbb-R-2)在三维实空间中的旋转(Rotations in$$

)
<!-- Image removed: CSDN link no longer accessible -->
可以这样理解，先固定一个坐标轴，然后从上往下看去，得到这个向量在另外两个基向量所形成的向量空间中正交投影，然后再作相应的旋转操作。

关于的旋转操作：

$$\bold R_1(\theta)=\left[\begin{array}{c}    \Phi(\bold e_1)&\Phi(\bold e_2) &\Phi(\bold e_3)   \end{array}\right]=\left[\begin{array}{c} 1&0&0 \\0&\cos\theta&-\sin\theta\\0&\sin\theta&\cos\theta       \end{array}\right]$$

类似的，只要固定哪个坐标轴，哪个坐标轴就是基向量。

### [](#%E5%9C%A8-mathcal-n-%E7%BB%B4%E7%A9%BA%E9%97%B4%E4%B8%AD%E7%9A%84%E6%97%8B%E8%BD%AC)在

维空间中的旋转
**吉文斯旋转**（Givens Rotation）：
<!-- Image removed: CSDN link no longer accessible -->
实际上就是等价于单位矩阵对应位置上变成一个正弦或者余弦值。

### [](#%E6%97%8B%E8%BD%AC%E7%9A%84%E7%89%B9%E6%80%A7)旋转的特性

简单来说就是变换之后向量之间的距离角度不变，三维及三维以上的旋转操作不满足交换律，二维的满足。
<!-- Image removed: CSDN link no longer accessible -->