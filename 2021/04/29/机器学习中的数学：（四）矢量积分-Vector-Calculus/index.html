
 <!DOCTYPE HTML>
<html lang="en">
<head>
  <meta charset="UTF-8">
  
    <title>机器学习中的数学：（四）矢量积分(Vector Calculus) | Hexo</title>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=3, minimum-scale=1">
    
    <meta name="author" content="John Doe">
    
    <meta name="description" content="单变量微分（Differentiation of Univariate Functions）定义：差商形式正式定义：割线在极限情况下变成切线

多项式导数的推导：

\begin{aligned}
\frac{\mathrm{d} f}{\mathrm{~d} x} &amp;=\lim _{h \righ">
    
    
    
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/pacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/pacman.jpg">
    
    
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
</head>

  <body>
    <header>
      <div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.svg" alt="Hexo" title="Hexo"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Hexo">Hexo</a></h1>
				<h2 class="blog-motto"></h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="Menu">
			</a></div>
			<nav class="animated">
				<ul>
					
						<li><a href="/">Home</a></li>
					
						<li><a href="/archives">Archives</a></li>
					
					<li>
					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="text" id="search" name="q" autocomplete="off" maxlength="20" placeholder="Search" />
						<input type="hidden" name="q" value="site:Baymine.github.io">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main" class="post" itemscope itemprop="blogPost">
	<article itemprop="articleBody"> 
		<header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2021/04/29/机器学习中的数学：（四）矢量积分-Vector-Calculus/" title="机器学习中的数学：（四）矢量积分(Vector Calculus)" itemprop="url">机器学习中的数学：（四）矢量积分(Vector Calculus)</a>
  </h1>
  <p class="article-author">By
    
      <a href="https://Baymine.github.io" title="John Doe">John Doe</a>
    </p>
  <p class="article-time">
    <time datetime="2021-04-29T04:23:32.000Z" itemprop="datePublished">2021-04-29</time>
    Updated:<time datetime="2023-03-08T11:51:32.223Z" itemprop="dateModified">2023-03-08</time>
    
  </p>
</header>
	<div class="article-content">
		
		
		<div id="toc" class="toc-article">
			<strong class="toc-title">Contents</strong>
		<ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E5%BE%AE%E5%88%86%EF%BC%88Differentiation-of-Univariate-Functions%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">单变量微分（Differentiation of Univariate Functions）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">泰勒级数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%EF%BC%88Partial-Differentiation-and-Gradients%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">偏导数和梯度（Partial Differentiation and Gradients）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-of-Vector-Valued-Functions%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">向量值函数的梯度（Gradients of Vector-Valued Functions）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-of-Matrices%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">矩阵梯度（Gradients of Matrices）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E6%97%B6%E6%9C%89%E7%94%A8%E7%9A%84%E6%81%92%E7%AD%89%E5%BC%8F%EF%BC%88Useful-Identities-for-Computing-Gradients%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">计算梯度时有用的恒等式（Useful Identities for Computing Gradients）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%88Backpropagation-and-Automatic-Differentiation%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">反向传播和自动微分（Backpropagation and Automatic Differentiation）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-in-a-Deep-Network%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">深度网络的梯度（Gradients in a Deep Network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%88Automatic-Differentiation%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">自动微分（Automatic Differentiation）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%81%8F%E5%AF%BC%E6%95%B0%EF%BC%88Higher-Order-Derivatives%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">高阶偏导数（Higher-Order Derivatives）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8C%96%E5%92%8C%E5%A4%9A%E5%85%83%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0%EF%BC%88Linearization-and-Multivariate-Taylor-Series%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">线性化和多元泰勒级数（Linearization and Multivariate Taylor Series）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">9.</span> <span class="toc-text">补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89"><span class="toc-number">9.1.</span> <span class="toc-text">反向传播（推导）</span></a></li></ol></li></ol>
		</div>
		
		<p><img src="https://img-blog.csdnimg.cn/20210426135218277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="单变量微分（Differentiation-of-Univariate-Functions）"><a href="#单变量微分（Differentiation-of-Univariate-Functions）" class="headerlink" title="单变量微分（Differentiation of Univariate Functions）"></a>单变量微分（Differentiation of Univariate Functions）</h1><p><strong>定义：差商形式</strong><br><img src="https://img-blog.csdnimg.cn/20210426135329872.png?x-oss-process=image,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210426135407699.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>正式定义</strong>：<br>割线在极限情况下变成切线<br><img src="https://img-blog.csdnimg.cn/20210426135447455.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<blockquote>
<p>多项式导数的推导：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\mathrm{d} f}{\mathrm{~d} x} &=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}\\
&=\lim _{h \rightarrow 0} \frac{(x+h)^{n}-x^{n}}{h} \\
&=\lim _{h \rightarrow 0} \frac{\sum_{i=0}^{n}\left(\begin{array}{l}
n \\
i
\end{array}\right) x^{n-i} h^{i}-x^{n}}{h} .
\end{aligned}</script><p>由于$x^n=\left(\begin{array}{l}n \ 0 \end{array}\right)x^{n-0}h^0$所以：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\mathrm{d} f}{\mathrm{~d} x} &=\lim _{h \rightarrow 0} \frac{\sum_{i=1}^{n}\left(\begin{array}{l}
n \\
i
\end{array}\right) x^{n-i} h^{i}}{h} \\
&=\lim _{h \rightarrow 0} \sum_{i=1}^{n}\left(\begin{array}{c}
n \\
i
\end{array}\right) x^{n-i} h^{i-1} \\
&=\lim _{h \rightarrow 0}\left(\begin{array}{l}
n \\
1
\end{array}\right) x^{n-1}+\underbrace{\sum_{i=2}^{n}\left(\begin{array}{l}
n \\
i
\end{array}\right) x^{n-i} h^{i-1}}_{\rightarrow 0 \text { as } h \rightarrow 0} \\
&=\frac{n !}{1 !(n-1) !} x^{n-1}=n x^{n-1} .
\end{aligned}</script><p>其中的$\left(\begin{array}{l}<br>n \<br>i<br>\end{array}\right)$是组合数$C^i_n$</p>
</blockquote>
<h2 id="泰勒级数"><a href="#泰勒级数" class="headerlink" title="泰勒级数"></a>泰勒级数</h2><p><strong>泰勒多项式</strong>：<br><img src="https://img-blog.csdnimg.cn/20210426142816300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>对于$f\in\mathcal C^{\infty}, f:\mathbb R \rightarrow \mathcal R,f$在$x_0$的泰勒级数为:($f\in\mathcal C^{\infty}$表示$f$无穷多项都是是连续可微的，？)<br><img src="https://img-blog.csdnimg.cn/20210427101504784.png" alt="在这里插入图片描述"><br>当$x_0=0$时.称为<strong>麦克劳林级数(Maclaurin series)</strong><br>泰勒多项式表示对函数的一种近似,多项式的项越多,与原先的函数就越接近.下图中，$T_i$表示$f$的$i$项展开。<br><img src="https://img-blog.csdnimg.cn/2021042710221633.png?x-oss-process=image,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<blockquote>
<p>三角函数的泰勒展开：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\cos (x) &=\sum_{k=0}^{\infty}(-1)^{k} \frac{1}{(2 k) !} x^{2 k}, \\
\sin (x) &=\sum_{k=0}^{\infty}(-1)^{k} \frac{1}{(2 k+1) !} x^{2 k+1} .
\end{aligned}</script></blockquote>
<p>泰勒级数实际上是一种特殊的幂级数：</p>
<script type="math/tex; mode=display">f(x)=\sum^\infty_{k=0}a_k(x-c)^k,\quad 幂级数</script><p>一些求导法则：<br><img src="https://img-blog.csdnimg.cn/20210427102600815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h1 id="偏导数和梯度（Partial-Differentiation-and-Gradients）"><a href="#偏导数和梯度（Partial-Differentiation-and-Gradients）" class="headerlink" title="偏导数和梯度（Partial Differentiation and Gradients）"></a>偏导数和梯度（Partial Differentiation and Gradients）</h1><p>偏导数定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial f}{\partial x_{1}} &=\lim _{h \rightarrow 0} \frac{f\left(x_{1}+h, x_{2}, \ldots, x_{n}\right)-f(x)}{h} \\
& \vdots \\
\frac{\partial f}{\partial x_{n}} &=\lim _{h \rightarrow 0} \frac{f\left(x_{1}, \ldots, x_{n-1}, x_{n}+h\right)-f(\boldsymbol{x})}{h}
\end{aligned}</script><p>可以将函数对所有变量的偏导数写成一个行向量：</p>
<script type="math/tex; mode=display">
\nabla_{\boldsymbol{x}} f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{~d} \boldsymbol{x}}=\left[\begin{array}{llll}
\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} & \frac{\partial f(\boldsymbol{x})}{\partial x_{2}} & \cdots & \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}
\end{array}\right] \in \mathbb{R}^{1 \times n}</script><p>这个式子被称为$f$的<strong>梯度</strong>或者<strong>雅可比矩阵</strong>（Jacobian）。<br>对于一个多变量函数的偏导数（$f=(x_1,x_2),x_1=x_1(s，t),x_2=x_2(s,t)$）可以写成矩阵乘法的形式：</p>
<script type="math/tex; mode=display">
\frac{\mathrm{d} f}{\mathrm{~d}(s, t)}=\frac{\partial f}{\partial \boldsymbol{x}} \frac{\partial \boldsymbol{x}}{\partial(s, t)}=\underbrace{\left[\frac{\partial f}{\partial x_{1}} \quad \frac{\partial f}{\partial x_{2}}\right]}_{=\frac{\partial f}{\partial \boldsymbol{x}}} \underbrace{\left[\begin{array}{cc}
\frac{\partial x_{1}}{\partial s} & \frac{\partial x_{1}}{\partial t} \\
\frac{\partial x_{2}}{\partial s} & \frac{\partial x_{2}}{\partial t}
\end{array}\right]}_{=\frac{\partial \boldsymbol{x}}{\partial(s, t)}} .</script><blockquote>
<p>为了检验梯度计算结果的正确行，可以采用<strong>梯度验证</strong>（Gradient checking）的方式进行检验：<br>这里用到了<strong>有限差分法</strong>（Finite difference method，FDM）：FDM are one of the most common approaches to the numerical solution of PDE（partial differential equations）, along with finite element methods.<br>就是将连续函数离散化。</p>
<blockquote>
<p>FDM的基本原理就是利用一个很小的数$\epsilon(10^{-4})$，判断自变量在这个范围中变化时对应的函数值的变化情况是否与梯度相似。</p>
<script type="math/tex; mode=display">\frac{d}{d\theta}J(\theta)\approx \frac{J(\theta + \epsilon)-J(\theta-\epsilon)}{2\epsilon}, \quad \epsilon\rightarrow 0</script></blockquote>
</blockquote>
<h1 id="向量值函数的梯度（Gradients-of-Vector-Valued-Functions）"><a href="#向量值函数的梯度（Gradients-of-Vector-Valued-Functions）" class="headerlink" title="向量值函数的梯度（Gradients of Vector-Valued Functions）"></a>向量值函数的梯度（Gradients of Vector-Valued Functions）</h1><p><img src="https://img-blog.csdnimg.cn/20210427133447332.png?x-oss-process=image,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>这样，$\boldsymbol f$就将原先的$\boldsymbol x \in \mathbb R^n$映射成$\mathbb R^m$,对于每一个$\boldsymbol f_i：\mathbb R^n\rightarrow\mathbb R$,也就是将原先的n维自变量映射成了一个实数。<br>所以：</p>
<script type="math/tex; mode=display">\frac{\partial\boldsymbol f(x)}{\partial x_i}=\left[\begin{array}{cc}\frac{\partial f_1(x)}{\partial x_i}\\\vdots\\ \frac{\partial f_m(x)}{\partial x_i} \end{array}\right]\in \mathbb R^m</script><p>而一个函数对一个列向量的映射，也就是之前提到梯度，可以写成：</p>
<script type="math/tex; mode=display">
\nabla_{\boldsymbol{x}} f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{~d} \boldsymbol{x}}=\left[\begin{array}{llll}
\frac{\partial f(\boldsymbol{x})}{\partial x_{1}} & \frac{\partial f(\boldsymbol{x})}{\partial x_{2}} & \cdots & \frac{\partial f(\boldsymbol{x})}{\partial x_{n}}
\end{array}\right] \in \mathbb{R}^{1 \times n}</script><p>代入上式，得到向量值方程的一阶偏导数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J &=\nabla_{x} f=\frac{\mathrm{d} f(x)}{\mathrm{d} x}=\left[\begin{array}{ccc}
\frac{\partial f(x)}{\partial x_{1}} & \cdots & \frac{\partial f(x)}{\partial x_{n}}
\end{array}\right] \\
&=\left[\begin{array}{ccc}
\frac{\partial f_{1}(x)}{\partial x_{1}} & \cdots & \frac{\partial f_{1}(x)}{\partial x_{n}} \\
\vdots & & \vdots \\
\frac{\partial f_{m}(x)}{\partial x_{1}} & \cdots & \frac{\partial f_{m}(x)}{\partial x_{n}}
\end{array}\right] \\
x &=\left[\begin{array}{c}
x_{1} \\
\vdots \\
x_{n}
\end{array}\right], \quad J(i, j)=\frac{\partial f_{i}}{\partial x_{j}} .
\end{aligned}</script><p>$\boldsymbol f:\mathbb R^n\rightarrow\mathbb R^m$的一阶偏导数称为<strong>雅可比矩阵</strong>。<br>雅可比矩阵用于求解映射之后图形的<strong>比例因子</strong>（scaling factor）<br><img src="https://img-blog.csdnimg.cn/20210427140052382.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>想要找到比例因子，可以找出对应的变换矩阵，这个矩阵的行列式的绝对值就是面积变化的比例。但是这个适用于线性变换，当面对非线性变换的时候，需要采取另一种策略。<br>想要知道当$x$变化的时候$f(x)$的变化情况，我们可以使用偏导数得到变化信息。所以，雅可比矩阵可以表示相对应的变换矩阵。</p>
<blockquote>
<p>非线性的情况时，采用逼近的方式获得比例因子<br><img src="https://img-blog.csdnimg.cn/20210427195823520.png?x-oss-process=image,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<p>向量和对应的映射所处的维度与偏导数的关系：<br><img src="https://img-blog.csdnimg.cn/20210427200004258.png" alt="在这里插入图片描述"></p>
<h1 id="矩阵梯度（Gradients-of-Matrices）"><a href="#矩阵梯度（Gradients-of-Matrices）" class="headerlink" title="矩阵梯度（Gradients of Matrices）"></a>矩阵梯度（Gradients of Matrices）</h1><p>矩阵的梯度的结果可能得到一个高维的矩阵，这种矩阵称为<strong>张量</strong>（Tensor）<br>两种计算矩阵梯度的方法：<br>一种是直接计算，最后将结果拼装起来，另一种是将矩阵变成一个向量，</p>
<p><img src="https://img-blog.csdnimg.cn/20210427202543293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<blockquote>
<p><img src="https://img-blog.csdnimg.cn/20210427204602190.png" alt="在这里插入图片描述"></p>
<script type="math/tex; mode=display">\frac{d\boldsymbol K}{d\boldsymbol R}\in \mathbb R^{(N\times N)\times(M\times N)}</script><script type="math/tex; mode=display">\frac {d K_{pq}}{d\boldsymbol R}\in \mathbb R^{1\times(M\times N)}</script><script type="math/tex; mode=display">K_{pq}=r_p^\top r_q=\sum^M_{m=1}\boldsymbol R_{mq}\boldsymbol R_{mp}</script><script type="math/tex; mode=display">\frac{\partial \boldsymbol K_{pq}}{\partial \boldsymbol R_{ij}}=\sum_{m=1}^M\frac{\partial}{\partial R_{ij}}R_{mp}R_{mq}=\partial_{pqij},\partial_{}
\partial_{p q i j}=\left\{\begin{array}{ll}
R_{i q} & \text { if } j=p, p \neq q \\
R_{i p} & \text { if } j=q, p \neq q \\
2 R_{i q} & \text { if } j=p, p=q \\
0 & \text { otherwise }
\end{array} .\right.</script></blockquote>
<h1 id="计算梯度时有用的恒等式（Useful-Identities-for-Computing-Gradients）"><a href="#计算梯度时有用的恒等式（Useful-Identities-for-Computing-Gradients）" class="headerlink" title="计算梯度时有用的恒等式（Useful Identities for Computing Gradients）"></a>计算梯度时有用的恒等式（Useful Identities for Computing Gradients）</h1><script type="math/tex; mode=display">
\begin{aligned}
&\frac{\partial}{\partial \boldsymbol{X}} \boldsymbol{f} (\boldsymbol{X})^{\top}=\left(\frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right)^{\top}\\
&\frac{\partial}{\partial \boldsymbol{X}} \operatorname{tr}(\boldsymbol{f}(\boldsymbol{X}))=\operatorname{tr}\left(\frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right)\\
&\frac{\partial}{\partial \boldsymbol{X}} \operatorname{det}(\boldsymbol{f}(\boldsymbol{X}))=\operatorname{det}(\boldsymbol{f}(\boldsymbol{X})) \operatorname{tr}\left(\boldsymbol{f}(\boldsymbol{X})^{-1} \frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right)\\
&\frac{\partial}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}=-\boldsymbol{f}(\boldsymbol{X})^{-1} \frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}\\
&\frac{\partial \boldsymbol{a}^{\top} \boldsymbol{X}^{-1} \boldsymbol{b}}{\partial \boldsymbol{X}}=-\left(\boldsymbol{X}^{-1}\right)^{\top} \boldsymbol{a} b^{\top}\left(\boldsymbol{X}^{-1}\right)^{\top}\\
&\frac{\partial \boldsymbol{x}^{\top} \boldsymbol{a}}{\partial \boldsymbol{x}}=\boldsymbol{a}^{\top}\\
&\frac{\partial \boldsymbol{a}^{\top} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{a}^{\top}\\
&\frac{\partial \boldsymbol{a}^{\top} \boldsymbol{X} \boldsymbol{b}}{\partial \boldsymbol{X}}=\boldsymbol{a} \boldsymbol{b}^{\top}\\
&\frac{\partial \boldsymbol{x}^{\top} \boldsymbol{B} \boldsymbol{x}}{\partial \boldsymbol{x}}=\boldsymbol{x}^{\top}\left(\boldsymbol{B}+\boldsymbol{B}^{\top}\right)\\
&\frac{\partial}{\partial s}(x-A s)^{\top} W(x-A s)=-2(x-A s)^{\top} W A \quad \text { for symmetric } W\\
&(5.108)
\end{aligned}</script><blockquote>
<p>计算$\frac{\partial x^\top Bx}{\partial x}=x^\top(B+B^\top)$:</p>
</blockquote>
<h1 id="反向传播和自动微分（Backpropagation-and-Automatic-Differentiation）"><a href="#反向传播和自动微分（Backpropagation-and-Automatic-Differentiation）" class="headerlink" title="反向传播和自动微分（Backpropagation and Automatic Differentiation）"></a>反向传播和自动微分（Backpropagation and Automatic Differentiation）</h1><p>为了计算损失函数（Loss Function）的最小值，这时候需要对损失函数对其所有的参数求偏导，也就是求出损失函数的梯度。但是用传统的链式法则会使得中间步骤十分繁琐，所以有了<strong>反向传播算法</strong>（Backpropagation Algorithm)可以有效地解决损失函数的梯度的问题，并且运算速度与传统的链式法则的计算方式相同。</p>
<h2 id="深度网络的梯度（Gradients-in-a-Deep-Network）"><a href="#深度网络的梯度（Gradients-in-a-Deep-Network）" class="headerlink" title="深度网络的梯度（Gradients in a Deep Network）"></a>深度网络的梯度（Gradients in a Deep Network）</h2><p>在深度学习中，一个函数通常是由许多的函数复合而成的。</p>
<script type="math/tex; mode=display">\boldsymbol{y}=\left(f_{K} \circ f_{K-1} \circ \cdots \circ f_{1}\right)(\boldsymbol{x})=f_{K}\left(f_{K-1}\left(\cdots\left(f_{1}(\boldsymbol{x})\right) \cdots\right)\right)</script><p>后一层神经元会使用前一层神经元的输出值作为该层的输入值，所以有：$f<em>i(x</em>{i-1})=\sigma(\boldsymbol A<em>{i-1}x</em>{i-1}+b<em>{i-1}),\quad \sigma$ 为激活函数.$x</em>{i-1}$是第i层的输出值。</p>
<p><img src="https://img-blog.csdnimg.cn/20210428141227595.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>为了训练这个神经网络，我们需要求解出损失函数梯度。<br>假设：</p>
<script type="math/tex; mode=display">f_{0}:=x \\ f_{i}:=\sigma_{i}\left(A_{i-1} f_{i-1}+b_{i-1}\right), \quad i=1, \ldots, K</script><p>损失函数为：</p>
<script type="math/tex; mode=display">L(\theta)=\|y-f_K(\theta,x)\|^2,\quad \theta=\{\boldsymbol A_0,\boldsymbol b_0,\dots, \boldsymbol A_{K-1},\boldsymbol b_{K-1}\}</script><p>要求解这个函数的最小值，我们需要对损失函数求偏导。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial L}{\partial \boldsymbol{\theta}_{K-1}} &=\frac{\partial L}{\partial \boldsymbol{f}_{K}} \frac{\partial \boldsymbol{f}_{K}}{\partial \boldsymbol{\theta}_{K-1}} \\
\frac{\partial L}{\partial \boldsymbol{\theta}_{K-2}} &=\frac{\partial L}{\partial \boldsymbol{f}_{K}}\frac{\partial f_{K}}{\partial f_{K-1}} \frac{\partial \boldsymbol{f}_{K-1}}{\partial \boldsymbol{\theta}_{K-2}}\\
\frac{\partial L}{\partial \boldsymbol{\theta}_{K-3}} &=\frac{\partial L}{\partial \boldsymbol{f}_{K}} \frac{\partial f_{K}}{\partial f_{K-1}} \frac{\partial \boldsymbol{f}_{K-1}}{\partial f_{K-2}} \frac{\partial \boldsymbol{f}_{K-2}}{\partial \boldsymbol{\theta}_{K-3}} \\
\frac{\partial L}{\partial \boldsymbol{\theta}_{i}} &=\frac{\partial L}{\partial \boldsymbol{f}_{K}} \frac{\partial f_{K}}{\partial f_{K-1}} \cdots \frac{\partial f_{i+2}}{\partial f_{i+1}} \frac{\partial \boldsymbol{f}_{i+1}}{\partial \boldsymbol{\theta}_{i}}
\end{aligned}</script><p>这样看来，当我们需要计算$\frac{\partial L}{\partial\boldsymbol \theta<em>i}$时，我们可以利用之前的$\frac{\partial L}{\partial\boldsymbol \theta</em>{i+1}}$简化计算。</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial \boldsymbol\theta_i}=\frac{\partial L}{\partial \boldsymbol\theta_{i+1}}\frac{\partial \boldsymbol f_{i+1}}{\partial \boldsymbol\theta_i}</script><p><img src="https://img-blog.csdnimg.cn/20210428143935866.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="自动微分（Automatic-Differentiation）"><a href="#自动微分（Automatic-Differentiation）" class="headerlink" title="自动微分（Automatic Differentiation）"></a>自动微分（Automatic Differentiation）</h2><p>反向传播算法实际上时自动微分中的一个特例。自动微分类似计算的时候用的还原法，将这些中间步骤用一个变量表示出来：</p>
<script type="math/tex; mode=display">y=f(g(h(x)))=f(g(h(w_0)))=f(g(w_1))=f(w_2)=w_3</script><p>原始的链式法则：</p>
<script type="math/tex; mode=display">\frac {dy}{dx}=\frac{dy}{dw_2}\frac{dw_2}{dw_1}\frac{dw_1}{dx}=\frac{df(w_2)}{dw_2}\frac{dg(w_1)}{dw_1}\frac{dh(w_0)}{dx}</script><p>自动微分有两种模式：<strong>向前模式</strong>（forward mode）和<strong>向后模式</strong>（reverse mode）<br>向前模式就是从内层函数到外层函数逐步进行求导，向后模式则是相反。</p>
<p>使用计算图（computational graphs），每个节点代表一个计算过程中的中间变量。<br>例如：$f(x)=\sqrt{x^{2}+\exp \left(x^{2}\right)}+\cos \left(x^{2}+\exp \left(x^{2}\right)\right)$用计算图可以表示为：<br><img src="https://img-blog.csdnimg.cn/20210428212906625.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTMxNTY1Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>可以描述为：</p>
<script type="math/tex; mode=display">For\ \ i=d+1,\dots, D :\quad x_i=g_i(x_{pa(x_i)})</script><p>其中，$x_{Pa(x_i)}$表示节点$x_i$的父节点。$g_i(\cdot)$表示节点对应的计算函数。</p>
<blockquote>
<p>这部分没什么弄懂，后续继续补充。<br>计算图的那个部分。<br>利用上面的关系可以得出：</p>
<script type="math/tex; mode=display">\frac{df}{dx_i}=\sum_{j:i\in Pa(j)}\frac{df}{dx_i}\frac{dx_j}{dx_i}=\sum_{j:i\in Pa(j)}\frac{df}{dx_j}\frac{dg_i}{dx_i}</script><p>这实际上就是利用链式法则求解出对中间变量的微分。对最后一个中间变量的微分为1</p>
<p>符号微分（Symbolic differentiation）之所以复杂是因为在运算过程中并没有中间变量，所以想要直接编码解决难度较大</p>
</blockquote>
<h1 id="高阶偏导数（Higher-Order-Derivatives）"><a href="#高阶偏导数（Higher-Order-Derivatives）" class="headerlink" title="高阶偏导数（Higher-Order Derivatives）"></a>高阶偏导数（Higher-Order Derivatives）</h1><p>当我们想要用牛顿法进行优化的时候，二阶偏导数就不得不被使用了。<br>有一个符号需要注意：<br>$\frac {\partial^2 f}{\partial x\partial y}$这个意思时先对y求导，然后再对x求导。<br><strong>海森矩阵</strong>（Hessian Matrix）<br>海森矩阵存储函数的二阶偏导数。</p>
<script type="math/tex; mode=display">\nabla^2_{x,y}f(x,y)=\boldsymbol{H}=\left[\begin{array}{cc}\frac{\partial^{2} f}{\partial x^{2}} & \frac{\partial^{2} f}{\partial x \partial y} \\ \frac{\partial^{2} f}{\partial x \partial y} & \frac{\partial^{2} f}{\partial y^{2}}\end{array}\right]</script><p>表示函数在$(x,y)$处的曲率</p>
<h1 id="线性化和多元泰勒级数（Linearization-and-Multivariate-Taylor-Series）"><a href="#线性化和多元泰勒级数（Linearization-and-Multivariate-Taylor-Series）" class="headerlink" title="线性化和多元泰勒级数（Linearization and Multivariate Taylor Series）"></a>线性化和多元泰勒级数（Linearization and Multivariate Taylor Series）</h1><p>假设一个函数：</p>
<script type="math/tex; mode=display">\begin{aligned} f: \mathbb{R}^{D} & \rightarrow \mathbb{R} \\ \quad \boldsymbol{x} & \mapsto f(\boldsymbol{x}), \quad \boldsymbol{x} \in \mathbb{R}^{D} \end{aligned}</script><p>在$x_0$处光滑，设$\delta := x-x_0$,所以：</p>
<script type="math/tex; mode=display">f(x)=\sum^\infty_{k=0}\frac{D^k_\boldsymbol xf(x_0)}{k!}\delta^k</script><p>为$f(x)$在$x<em>0$处的多元泰勒公式。其中,$D^k</em>\boldsymbol xf(x<em>0)$，表示$f(x)$对x的k阶偏导。$D^k</em>\boldsymbol xf(x_0)$和$\delta^k$都是k阶张量，其中：</p>
<script type="math/tex; mode=display">
\boldsymbol{\delta}^{k} \in \mathbb{R} \overbrace{D \times D \times \ldots \times D}^{k \text { times }}</script><script type="math/tex; mode=display">\boldsymbol{\delta}^{3}:=\boldsymbol{\delta} \otimes \boldsymbol{\delta} \otimes \boldsymbol{\delta}, \quad \boldsymbol{\delta}^{3}[i, j, k]=\delta[i] \delta[j] \delta[k]</script><p>所以，（第一个中括号是前面偏导向量的索引）</p>
<script type="math/tex; mode=display">D_{\boldsymbol{x}}^{k} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{k}=\sum_{i_{1}=1}^{D} \cdots \sum_{i_{k}=1}^{D} D_{\boldsymbol{x}}^{k} f\left(\boldsymbol{x}_{0}\right)\left[i_{1}, \ldots, i_{k}\right] \delta\left[i_{1}\right] \cdots \delta\left[i_{k}\right]</script><p>下面是上式的前三项：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
k=0, \ldots, 3 \text { and } \delta:=x-x_{0}: \\
k=0: D_{\boldsymbol{x}}^{0} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{0}=f\left(\boldsymbol{x}_{0}\right) \in \mathbb{R} \\
k=1: D_{\boldsymbol{x}}^{1} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{1}=\underbrace{\nabla_{\boldsymbol{x}} f\left(\boldsymbol{x}_{0}\right)}_{1 \times D} \underbrace{\delta}_{D \times 1}=\sum_{i=1}^{D} \nabla_{\boldsymbol{x}} f\left(\boldsymbol{x}_{0}\right)[i] \delta[i] \in \mathbb{R} \\
k=2: D_{\boldsymbol{x}}^{2} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{2}=\operatorname{tr}(\underbrace{\boldsymbol{H}\left(\boldsymbol{x}_{0}\right)}_{D \times D} \underbrace{\delta}_{D \times 1} \underbrace{\delta^{\top}}_{1 \times D})=\delta^{\top} \boldsymbol{H}\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta} \\
=\sum_{i=1}^{D} \sum_{j=1}^{D} H[i, j] \delta[i] \delta[j] \in \mathbb{R} \\
k=3: D_{\boldsymbol{x}}^{3} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{3}=\sum_{i=1}^{D} \sum_{j=1}^{D} \sum_{k=1}^{D} D_{x}^{3} f\left(\boldsymbol{x}_{0}\right)[i, j, k] \delta[i] \delta[j] \delta[k] \in \mathbb{R}
\end{array}</script><p>其中$\boldsymbol H(x_0)$表示在$x_0$处的海森矩阵。</p>
<blockquote>
<p>证明？</p>
<script type="math/tex; mode=display">k=2: D_{\boldsymbol{x}}^{2} f\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta}^{2}=\operatorname{tr}(\underbrace{\boldsymbol{H}\left(\boldsymbol{x}_{0}\right)}_{D \times D} \underbrace{\delta}_{D \times 1} \underbrace{\delta^{\top}}_{1 \times D})=\delta^{\top} \boldsymbol{H}\left(\boldsymbol{x}_{0}\right) \boldsymbol{\delta} \\
=\sum_{i=1}^{D} \sum_{j=1}^{D} H[i, j] \delta[i] \delta[j] \in \mathbb{R}</script></blockquote>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="反向传播（推导）"><a href="#反向传播（推导）" class="headerlink" title="反向传播（推导）"></a>反向传播（推导）</h2><p>我们想要求的是对损失函数对参数的求导的结果：</p>
<script type="math/tex; mode=display">\frac{dL}{dv_i},i\ge N-M+1</script><p>利用链式法则：</p>
<script type="math/tex; mode=display">\frac{dL}{dv_i}=\sum_{j:i\in Pa(j)}\frac{dL}{dv_i}\frac{dv_i}{dv_j}</script><p>回想我们计算激活值的方法：</p>
<script type="math/tex; mode=display">v_i=\sigma_i(w_i\cdot v_{Pa(i)})</script><p>所以我们可以计算：</p>
<script type="math/tex; mode=display">\frac{dv_i}{dv_j}=\sigma_i'(\boldsymbol w_i\cdot \boldsymbol v_{Pa(i)})w_{iq},Pa(i)_q=j</script><p>举个例子，假设$Pa(i)=(2,7,9)$,则激活值为：</p>
<script type="math/tex; mode=display">v_i=\sigma_i(\boldsymbol w_i\cdot \boldsymbol v_{(2,7,9)})</script><p>展开即为下面这些式子：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{d v_{i}}{d v_{2}} &=\sigma_{i}^{\prime}\left(\mathbf{w}_{i} \cdot \mathbf{v}_{(2,7,9)}\right) w_{i 1} \\
\frac{d v_{i}}{d v_{7}} &=\sigma_{i}^{\prime}\left(\mathbf{w}_{i} \cdot \mathbf{v}_{(2,7,9)}\right) w_{i 2} \\
\frac{d v_{i}}{d v_{9}} &=\sigma_{i}^{\prime}\left(\mathbf{w}_{i} \cdot \mathbf{v}_{(2,7,9)}\right) w_{i 3} .
\end{aligned}</script><p>我们用$v<em>i’=\sigma’_i(\boldsymbol w_i\cdot \boldsymbol v</em>{Pa(i)})$带入到原先的式子中：</p>
<script type="math/tex; mode=display">\frac{dv_i}{dv_j}=v_i'w_{iq},Pa(i)_q=j</script><p>对应的向量形式为：</p>
<script type="math/tex; mode=display">\frac{dv_i}{d\mathbf v_{Pa(i)}}= v'_i\mathbf w_i</script><p>于是我们可以很容易得到：(带入已知式)</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{d L}{d \mathbf{w}_{i}} &=\frac{d L}{d v_{i}} \frac{d v_{i}}{d \mathbf{w}_{i}} \\
&=\frac{d L}{d v_{i}} \sigma_{i}^{\prime}\left(\mathbf{w} \cdot \mathbf{v}_{\mathrm{Pa}(i)}\right) \mathbf{v}_{\mathrm{Pa}(i)} \\
&=\frac{d L}{d v_{i}} v_{i}^{\prime} \mathbf{v}_{\mathrm{Pa}(i)}
\end{aligned}</script>  
	</div>
		<footer class="article-footer clearfix">




<div class="article-share" id="share">

  <div data-url="https://baymine.github.io/2021/04/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%EF%BC%9A%EF%BC%88%E5%9B%9B%EF%BC%89%E7%9F%A2%E9%87%8F%E7%A7%AF%E5%88%86-Vector-Calculus/" data-title="机器学习中的数学：（四）矢量积分(Vector Calculus) | Hexo" data-tsina="" class="share clearfix">
  </div>

</div>
</footer>   	       
	</article>
	
<nav class="article-nav clearfix">
 
 <div class="prev" >
 <a href="/2021/05/11/机器学习中的数学：（五）概率与分布-Probability-and-Distributions/" title="机器学习中的数学：（五）概率与分布(Probability and Distributions)">
  <strong>PREVIOUS:</strong><br/>
  <span>
  机器学习中的数学：（五）概率与分布(Probability and Distributions)</span>
</a>
</div>


<div class="next">
<a href="/2021/04/26/机器学习中的数学：（三）矩阵分解-Matrix-Decompositions/"  title="机器学习中的数学：（三）矩阵分解(Matrix Decompositions)">
 <strong>NEXT:</strong><br/> 
 <span>机器学习中的数学：（三）矩阵分解(Matrix Decompositions)
</span>
</a>
</div>

</nav>

	
</div>  
      <div class="openaside"><a class="navbutton" href="#" title="Show Sidebar"></a></div>

  <div id="toc" class="toc-aside">
  <strong class="toc-title">Contents</strong>
  <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E5%BE%AE%E5%88%86%EF%BC%88Differentiation-of-Univariate-Functions%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">单变量微分（Differentiation of Univariate Functions）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">泰勒级数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6%EF%BC%88Partial-Differentiation-and-Gradients%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">偏导数和梯度（Partial Differentiation and Gradients）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-of-Vector-Valued-Functions%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">向量值函数的梯度（Gradients of Vector-Valued Functions）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-of-Matrices%EF%BC%89"><span class="toc-number">4.</span> <span class="toc-text">矩阵梯度（Gradients of Matrices）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E6%97%B6%E6%9C%89%E7%94%A8%E7%9A%84%E6%81%92%E7%AD%89%E5%BC%8F%EF%BC%88Useful-Identities-for-Computing-Gradients%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">计算梯度时有用的恒等式（Useful Identities for Computing Gradients）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%88Backpropagation-and-Automatic-Differentiation%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">反向传播和自动微分（Backpropagation and Automatic Differentiation）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6%EF%BC%88Gradients-in-a-Deep-Network%EF%BC%89"><span class="toc-number">6.1.</span> <span class="toc-text">深度网络的梯度（Gradients in a Deep Network）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%EF%BC%88Automatic-Differentiation%EF%BC%89"><span class="toc-number">6.2.</span> <span class="toc-text">自动微分（Automatic Differentiation）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%AB%98%E9%98%B6%E5%81%8F%E5%AF%BC%E6%95%B0%EF%BC%88Higher-Order-Derivatives%EF%BC%89"><span class="toc-number">7.</span> <span class="toc-text">高阶偏导数（Higher-Order Derivatives）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%8C%96%E5%92%8C%E5%A4%9A%E5%85%83%E6%B3%B0%E5%8B%92%E7%BA%A7%E6%95%B0%EF%BC%88Linearization-and-Multivariate-Taylor-Series%EF%BC%89"><span class="toc-number">8.</span> <span class="toc-text">线性化和多元泰勒级数（Linearization and Multivariate Taylor Series）</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">9.</span> <span class="toc-text">补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%EF%BC%88%E6%8E%A8%E5%AF%BC%EF%BC%89"><span class="toc-number">9.1.</span> <span class="toc-text">反向传播（推导）</span></a></li></ol></li></ol>
  </div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="Hide Sidebar"></a></div>
<aside class="clearfix">

  

  
<div class="tagslist">
	<p class="asidetitle">Tags</p>
		<ul class="clearfix">
		
			<li><a href="/tags/C/" title="C++">C++<sup>1</sup></a></li>
		
			<li><a href="/tags/C-primer/" title="C++ primer">C++ primer<sup>1</sup></a></li>
		
			<li><a href="/tags/DB/" title="DB">DB<sup>1</sup></a></li>
		
			<li><a href="/tags/OS/" title="OS">OS<sup>2</sup></a></li>
		
			<li><a href="/tags/Projects/" title="Projects">Projects<sup>1</sup></a></li>
		
			<li><a href="/tags/bugs/" title="bugs">bugs<sup>1</sup></a></li>
		
			<li><a href="/tags/computer-network/" title="computer network">computer network<sup>2</sup></a></li>
		
			<li><a href="/tags/侯捷C/" title="侯捷C++">侯捷C++<sup>5</sup></a></li>
		
		</ul>
</div>


  <div class="rsspart">
	<a href="" target="_blank" title="rss">RSS</a>
</div>

</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<div class="social-font clearfix">
		
		
		
		
		
	</div>
		<p class="copyright">Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/A-limon/pacman" target="_blank" title="Pacman">Pacman</a> © 2023 
		
		<a href="https://Baymine.github.io" target="_blank" title="John Doe">John Doe</a>
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.1.0.min.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else
    {
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
      
      $('#toc.toc-aside').css('display', 'none');
        
    }
  });
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
});
</script>

<script type="text/javascript">
$(document).ready(function(){ 
  var ai = $('.article-content>iframe'),
      ae = $('.article-content>embed'),
      t  = $('#toc'),
      h  = $('article h2')
      ah = $('article h2'),
      ta = $('#toc.toc-aside'),
      o  = $('.openaside'),
      c  = $('.closeaside');
  if(ai.length>0){
    ai.wrap('<div class="video-container" />');
  };
  if(ae.length>0){
   ae.wrap('<div class="video-container" />');
  };
  if(ah.length==0){
    t.css('display','none');
  }else{
    c.click(function(){
      ta.css('display', 'block').addClass('fadeIn');
    });
    o.click(function(){
      ta.css('display', 'none');
    });
    $(window).scroll(function(){
      ta.css("top",Math.max(140,320-$(this).scrollTop()));
    });
  };
});
</script>


<script type="text/javascript">
$(document).ready(function(){ 
  var $this = $('.share'),
      url = $this.attr('data-url'),
      encodedUrl = encodeURIComponent(url),
      title = $this.attr('data-title'),
      tsina = $this.attr('data-tsina');
  var html = [
  '<a href="#" class="overlay" id="qrcode"></a>',
  '<div class="qrcode clearfix"><span>扫描二维码分享到微信朋友圈</span><a class="qrclose" href="#share"></a><strong>Loading...Please wait</strong><img id="qrcode-pic" data-src="http://s.jiathis.com/qrcode.php?url=' + encodedUrl + '"/></div>',
  '<a href="#textlogo" class="article-back-to-top" title="Top"></a>',
  '<a href="https://www.facebook.com/sharer.php?u=' + encodedUrl + '" class="article-share-facebook" target="_blank" title="Facebook"></a>',
  '<a href="#qrcode" class="article-share-qrcode" title="QRcode"></a>',
  '<a href="https://twitter.com/intent/tweet?url=' + encodedUrl + '" class="article-share-twitter" target="_blank" title="Twitter"></a>',
  '<a href="http://service.weibo.com/share/share.php?title='+title+'&url='+encodedUrl +'&ralateUid='+ tsina +'&searchPic=true&style=number' +'" class="article-share-weibo" target="_blank" title="Weibo"></a>',
  '<span title="Share to"></span>'
  ].join('');
  $this.append(html);
  $('.article-share-qrcode').click(function(){
    var imgSrc = $('#qrcode-pic').attr('data-src');
    $('#qrcode-pic').attr('src', imgSrc);
    $('#qrcode-pic').load(function(){
        $('.qrcode strong').text(' ');
    });
  });
});     
</script>






  </body>
</html>
